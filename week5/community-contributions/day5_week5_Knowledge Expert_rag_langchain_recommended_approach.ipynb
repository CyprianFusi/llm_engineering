{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfe37963-1af6-44fc-a841-8e462443f5e6",
   "metadata": {},
   "source": [
    "## Expert Knowledge Worker\n",
    "\n",
    "* **A question answering agent** that is an expert knowledge worker\n",
    "* **To be used by employees of Insurellm**, an Insurance Tech company\n",
    "* The **agent needs to be accurate** and the **solution should be low cost**.\n",
    "\n",
    "This project will use **RAG (Retrieval Augmented Generation)** to ensure our question/answering assistant has high accuracy.\n",
    "\n",
    "### Sidenote: Business applications of this week's projects\n",
    "\n",
    "**RAG is perhaps the most immediately applicable technique of anything that we cover in the course!** In fact, there are commercial products that do precisely what we build this week: nuanced querying across large databases of information, such as company contracts or product specs. RAG gives you a quick-to-market, low cost mechanism for adapting an LLM to your business area.\n",
    "\n",
    "### Imports for langchain and Chroma and plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ba2779af-84ef-4227-9e9e-6eaf0df87e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from glob import glob\n",
    "from dotenv import load_dotenv\n",
    "import gradio as gr\n",
    "\n",
    "from langchain.document_loaders import DirectoryLoader, TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.schema import Document\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.memory import ConversationBufferMemory, ChatMessageHistory\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.manifold import TSNE\n",
    "import plotly.graph_objects as go\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = \"notebook\"    # I'm using my machine, not colab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd6d18d-a5ba-49b4-881f-59cd7ff512d7",
   "metadata": {},
   "source": [
    "### Load environment variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "802137aa-8a74-45e0-a487-d1974927d7ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)\n",
    "anthropic_api_key = os.environ['ANTHROPIC_API_KEY'] \n",
    "openai_api_key = os.environ['OPENAI_API_KEY'] \n",
    "huggingface_api_key = os.environ['HUGGINGFACE_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58c85082-e417-4708-9efe-81a5d55d1424",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = \"gpt-4o-mini\"\n",
    "db_name = \"vector_db\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16db359-daf2-42e6-a44c-63af06cdcb5d",
   "metadata": {},
   "source": [
    "## Read in documents using LangChain's loaders and add metadata to the splits\n",
    "Take everything in all the sub-folders of our knowledgebase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "730711a9-6ffe-4eee-8f48-d6cfb7314905",
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = glob(\"knowledge-base/*\")\n",
    "\n",
    "def add_metadata(doc, doc_type):\n",
    "    doc.metadata[\"doc_type\"] = doc_type\n",
    "    return doc\n",
    "\n",
    "text_loader_kwargs = {'encoding': 'utf-8'}\n",
    "documents = []\n",
    "for folder in folders:\n",
    "    doc_type = os.path.basename(folder)\n",
    "    loader = DirectoryLoader(folder, glob=\"**/*.md\", loader_cls=TextLoader, loader_kwargs=text_loader_kwargs)\n",
    "    folder_docs = loader.load()\n",
    "    documents.extend([add_metadata(doc, doc_type) for doc in folder_docs])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c9aad0-74dc-4631-afda-49fd7677fe23",
   "metadata": {},
   "source": [
    "### Split the documents into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31366b77-80af-40cd-ad58-563b6c609f3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Created a chunk of size 1088, which is longer than the specified 1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of chunks: 123\n",
      "Document types found: {'products', 'employees', 'contracts', 'company'}\n"
     ]
    }
   ],
   "source": [
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"Total number of chunks: {len(chunks)}\")\n",
    "print(f\"Document types found: {set(doc.metadata['doc_type'] for doc in documents)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77f7d2a6-ccfa-425b-a1c3-5e55b23bd013",
   "metadata": {},
   "source": [
    "## A sidenote on Embeddings, and \"Auto-Encoding LLMs\"\n",
    "\n",
    "* We will be mapping each chunk of text into a Vector that represents the **meaning of the text**, known as an **embeddings**.\n",
    "\n",
    "* `OpenAI` offers a model to do this, which we will use by calling their API with some `LangChain` code.\n",
    "\n",
    "* This model is an example of an **\"Auto-Encoding LLM\"** which generates an output given a complete input.\n",
    "It's different to all the other LLMs we've discussed today, which are known as **\"Auto-Regressive LLMs\"**, and generate **future tokens based only on past context**.\n",
    "\n",
    "Another example of an **Auto-Encoding LLMs** is `BERT` from Google. In addition to embedding, **Auto-encoding LLMs are often used for classification**.\n",
    "\n",
    "### Sidenote\n",
    "In week 8 we will return to **RAG and vector embeddings**, and we will use an **open-source vector encoder** so that the **data never leaves our computer** - that's an important consideration when building enterprise systems and the data needs to remain internal.\n",
    "\n",
    "### Put the chunks of data into a Vector Store that associates a Vector Embedding with each chunk\n",
    "If you would rather use the **free Vector Embeddings from HuggingFace sentence-transformers**, then replace `embeddings = OpenAIEmbeddings()` with:\n",
    "\n",
    "```python\n",
    "from langchain.embeddings import HuggingFaceEmbeddings`\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78998399-ac17-4e28-b15f-0b5f51e6ee23",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fb0bae6-6a03-4494-a6f4-c88ce285f0a5",
   "metadata": {},
   "source": [
    "### Create vectorstore\n",
    "Check if a **Chroma Datastore** already exists - if so, delete the collection to start from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e3cd723-131b-4b6c-9fb3-ab632c933e2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorstore created with 123 documents\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(db_name):\n",
    "    Chroma(persist_directory=db_name, embedding_function=embeddings).delete_collection()\n",
    "\n",
    "vectorstore = Chroma.from_documents(documents=chunks, embedding=embeddings, persist_directory=db_name)\n",
    "print(f\"Vectorstore created with {vectorstore._collection.count()} documents\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "077e9cdf-67d7-45a2-a8b2-32701e9c17f9",
   "metadata": {},
   "source": [
    "### Let's investigate the vectors\n",
    "Get one vector and find how many dimensions it has."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff2e7687-60d4-4920-a1d7-a34b9f70a250",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 123 vectors with 1,536 dimensions in the vector store\n"
     ]
    }
   ],
   "source": [
    "collection = vectorstore._collection\n",
    "count = collection.count()\n",
    "\n",
    "sample_embedding = collection.get(limit=1, include=[\"embeddings\"])[\"embeddings\"][0]\n",
    "dimensions = len(sample_embedding)\n",
    "print(f\"There are {count:,} vectors with {dimensions:,} dimensions in the vector store\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9468860b-86a2-41df-af01-b2400cc985be",
   "metadata": {},
   "source": [
    "## Time to use LangChain to bring it all together\n",
    "### Create a new Chat with OpenAI\n",
    "* set up the **conversation memory** for the chat\n",
    "* create the **retriever**: this is an **abstraction over the VectorStore** that will be used during RAG\n",
    "* putting it together: set up the **conversation chain with the `GPT 4o-mini LLM`, the `vector store` and `memory`**\n",
    "\n",
    "### The new and recommended way by Langchain \n",
    "* Use `RunnableWithMessageHistory` for memory\n",
    "* Directly pass the `retriever` and `LLM`; **memory is managed outside the chain**!\n",
    "\n",
    "### Set up memory and Create the chain with memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5716c66c-c24e-4d69-86d9-136cb0d1a767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define LLM, retriever and conversation chain\n",
    "llm = ChatOpenAI(temperature=0.7, model_name=MODEL)\n",
    "retriever = vectorstore.as_retriever()\n",
    "conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever)\n",
    "\n",
    "# Create Message History Factory\n",
    "message_histories = {}\n",
    "def get_message_history(session_id):\n",
    "    if session_id not in message_histories:\n",
    "        message_histories[session_id] = ChatMessageHistory()\n",
    "    return message_histories[session_id]\n",
    "\n",
    "# Wrap Chain with Memory\n",
    "chain_with_history = RunnableWithMessageHistory(\n",
    "    conversation_chain,\n",
    "    get_message_history,\n",
    "    input_messages_key=\"question\",\n",
    "    history_messages_key=\"chat_history\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b990455d-2c93-436a-a054-1c8b9cfd1f39",
   "metadata": {},
   "source": [
    "### Use the chain in RAG\n",
    "Query your documents and print the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cac3b67a-fef5-497c-9be4-464620b15dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "session_id = \"user-session-id\"\n",
    "config = {\"configurable\": {\"session_id\": session_id}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "281927a9-b317-4302-ac2d-f93692373cef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insurellm is an innovative insurance tech startup founded in 2015 by Avery Lancaster, focused on disrupting the insurance industry with its unique products. The company offers four main software products: Carllm for auto insurance, Homellm for home insurance, Rellm for the reinsurance sector, and Marketllm, a marketplace connecting consumers with insurance providers. With 200 employees and over 300 clients worldwide, Insurellm is committed to providing reliable and innovative solutions in the insurance tech space.\n"
     ]
    }
   ],
   "source": [
    "query = 'Can you describe Insurellm in a few sentences'\n",
    "result = chain_with_history.invoke({\"question\": query}, config = config)\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "de5e32c7-ea41-4d3b-9eef-71b93555f404",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Insurellm offers four insurance software products:\n",
      "\n",
      "1. Carllm - a portal for auto insurance companies\n",
      "2. Homellm - a portal for home insurance companies\n",
      "3. Rellm - an enterprise platform for the reinsurance sector\n",
      "4. Marketllm - a marketplace for connecting consumers with insurance providers\n"
     ]
    }
   ],
   "source": [
    "query = 'What products does Insurellm offer?'       \n",
    "result = chain_with_history.invoke({\"question\": query}, config = config)\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d3dfe99e-e062-432e-a18d-691e63379536",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I don't know.\n"
     ]
    }
   ],
   "source": [
    "query = 'Who received the prestigious IIOTY award in 2023?'       \n",
    "result = chain_with_history.invoke({\"question\": query}, config = config)\n",
    "print(result['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bc8736d-f56e-45f2-b932-e583a913e33b",
   "metadata": {},
   "source": [
    "The answer `\"I don't know.\"` is not correct. We'll investigate later...\n",
    "### Alternative\n",
    "if you'd like to use `Ollama` locally, you may use:\n",
    "\n",
    "`llm = ChatOpenAI(temperature=0.7, model_name='llama3.2', base_url='http://localhost:11434/v1', api_key='ollama')`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbcb659-13ce-47ab-8a5e-01b930494964",
   "metadata": {},
   "source": [
    "### User Interface: Using Gradio's `ChatInterface`\n",
    "A quick and easy way to prototype a chat with an LLM\n",
    "\n",
    "* set up a new conversation memory for the chat\n",
    "* putting it together: set up the conversation chain with the GPT 4o-mini LLM, the vector store and memory\n",
    "\n",
    "### A new conversation memory for the chat\n",
    "Instantiate chat history memory and add it to the chain."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd0e1b8-90ba-448a-89ea-c3ebbea197c4",
   "metadata": {},
   "source": [
    "### Wrapping that in a chat function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "489a802f-dc11-4bc6-8ec1-f3ba24dff416",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(question, history, session_id = \"user-session-id\"):\n",
    "    config = {\"configurable\": {\"session_id\": session_id}}\n",
    "    result = chain_with_history.invoke({\"question\": question}, config = config)\n",
    "    return result[\"answer\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef64a01b-e8ad-4541-a975-a16171bbedd5",
   "metadata": {},
   "source": [
    "### And in Gradio:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b252d8c1-61a8-406d-b57a-8f708a62b014",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7892\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7892/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "view = gr.ChatInterface(chat, type=\"messages\").launch(inbrowser=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2726eb83-eb78-4312-b6e6-df58d49011a1",
   "metadata": {},
   "source": [
    "### Create a new Chat with OpenAI\n",
    "Let's set the number of chunks to be analyzed to `25`, with the hope that the model would be able to response correctly to the query `\"Who received the prestigious IIOTY award in 2023?\"`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2136153b-d2f6-4c58-a0e3-78c3a932cf55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define LLM, retriever and conversation chain\n",
    "llm = ChatOpenAI(temperature=0.7, model_name=MODEL)\n",
    "retriever = vectorstore.as_retriever(search_kwargs={\"k\": 25})\n",
    "conversation_chain = ConversationalRetrievalChain.from_llm(llm=llm, retriever=retriever)\n",
    "\n",
    "# Create Message History Factory\n",
    "message_histories = {}\n",
    "def get_message_history(session_id):\n",
    "    if session_id not in message_histories:\n",
    "        message_histories[session_id] = ChatMessageHistory()\n",
    "    return message_histories[session_id]\n",
    "\n",
    "# Wrap Chain with Memory\n",
    "chain_with_history = RunnableWithMessageHistory(\n",
    "    conversation_chain,\n",
    "    get_message_history,\n",
    "    input_messages_key=\"question\",\n",
    "    history_messages_key=\"chat_history\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5c2bfa3c-810b-441b-90d1-31533f14b1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(question, history, session_id = \"user-session-id\"):\n",
    "    config = {\"configurable\": {\"session_id\": session_id}}\n",
    "    result = chain_with_history.invoke({\"question\": question}, config = config)\n",
    "    return result[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c736f33b-941e-4853-8eaf-2003bd988b18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7894\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7894/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "view = gr.ChatInterface(chat, type=\"messages\").launch(inbrowser=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
